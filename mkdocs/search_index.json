{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to NNSharp\n\n\nThis is the full documentation of NNSharp which is a lightweight library for running pre-trained neural networks. The training should be done in \nKeras\n, \nTensorFlow\n, \nPytorch\n or \nSonnet\n then the weights and the network architecture should be saved into a file (json). NNSharp is able to read and run the network especially on Windows and Visual Studio. \n\n\nCurrent abilities\n\n\nNNSharp recently supports Keras models with both Tensorflow and Theano backend. The list of the supported \nKeras layers\n:\n\n\n\n\nCore layers\n: Dense, Flatten, Reshape (2D), Permute, RepeatVector. \n\n\nConvolution layers\n: Conv1D, Conv2D, Cropping1D, Cropping2D. \n\n\nPooling layers\n: AveragePooling1D, AveragePooling2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D.\n\n\nActivations\n: softmax, elu, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid. \n\n\nRecurrent layers\n: SimpleRNN, LSTM, GRU.\n\n\nNormalization\n: BatchNormalization.\n\n\n\n\nThe \ndata format\n supports 2-dimensional data. \n\n\nThe structure of the documentation\n\n\nThe purpose of the documentation is twofold:\n\n\n\n\nAs a user guide to show how to use the package.\n\n\nProviding some further insight how the kernels work in a concise manner. Therefore developers and researchers can immediately understand what the kernel does without inspecting the code or looking for other sources on the net. This can be helpful for contributors as a developer documentation too. On the other hand users can understand what to feed into the network and how to use (and construct) the output.\n\n\n\n\nThe structure of the documentation is the following:\n\n\n\n\nInstallation\n shows how to get access to the package.\n\n\nModels\n introduces the model types and methods to access information about the model like the contained layers or nodes, execution time etc.\n\n\nKeras\n shows the implemented Keras layers. Each layer introduced with its input, output and its underlying process.\n\n\nTensorFlow\n shows how to use pre-trained TensorFlow models in NNSharp.\n\n\n\n\nFuture\n\n\nIn the future the library should provide the following features:\n\n\n\n\nSupports the whole Keras API.\n\n\nSupports TensorFlow models.\n\n\nSupports Sonnet.\n\n\nMulti-threading for faster kernels.\n\n\nKeras-like API over TensorFlow (and may be CNTK) for building, training and running networks. This requires an extended C++ API in TensorFlow and reliable compilation for Windows.\n\n\n\n\nContributions are always welcomed!\n For the current purposes see the \ngithub repository\n of the project.\n\n\nContact\n\n\nYou can do contact me via email if you have questions: adam.8.budai at gmail dot com",
            "title": "Overview"
        },
        {
            "location": "/#welcome-to-nnsharp",
            "text": "This is the full documentation of NNSharp which is a lightweight library for running pre-trained neural networks. The training should be done in  Keras ,  TensorFlow ,  Pytorch  or  Sonnet  then the weights and the network architecture should be saved into a file (json). NNSharp is able to read and run the network especially on Windows and Visual Studio.",
            "title": "Welcome to NNSharp"
        },
        {
            "location": "/#current-abilities",
            "text": "NNSharp recently supports Keras models with both Tensorflow and Theano backend. The list of the supported  Keras layers :   Core layers : Dense, Flatten, Reshape (2D), Permute, RepeatVector.   Convolution layers : Conv1D, Conv2D, Cropping1D, Cropping2D.   Pooling layers : AveragePooling1D, AveragePooling2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D.  Activations : softmax, elu, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid.   Recurrent layers : SimpleRNN, LSTM, GRU.  Normalization : BatchNormalization.   The  data format  supports 2-dimensional data.",
            "title": "Current abilities"
        },
        {
            "location": "/#the-structure-of-the-documentation",
            "text": "The purpose of the documentation is twofold:   As a user guide to show how to use the package.  Providing some further insight how the kernels work in a concise manner. Therefore developers and researchers can immediately understand what the kernel does without inspecting the code or looking for other sources on the net. This can be helpful for contributors as a developer documentation too. On the other hand users can understand what to feed into the network and how to use (and construct) the output.   The structure of the documentation is the following:   Installation  shows how to get access to the package.  Models  introduces the model types and methods to access information about the model like the contained layers or nodes, execution time etc.  Keras  shows the implemented Keras layers. Each layer introduced with its input, output and its underlying process.  TensorFlow  shows how to use pre-trained TensorFlow models in NNSharp.",
            "title": "The structure of the documentation"
        },
        {
            "location": "/#future",
            "text": "In the future the library should provide the following features:   Supports the whole Keras API.  Supports TensorFlow models.  Supports Sonnet.  Multi-threading for faster kernels.  Keras-like API over TensorFlow (and may be CNTK) for building, training and running networks. This requires an extended C++ API in TensorFlow and reliable compilation for Windows.   Contributions are always welcomed!  For the current purposes see the  github repository  of the project.",
            "title": "Future"
        },
        {
            "location": "/#contact",
            "text": "You can do contact me via email if you have questions: adam.8.budai at gmail dot com",
            "title": "Contact"
        },
        {
            "location": "/install/",
            "text": "Installation\n\n\n\n\nNNSharp is available as a \nnuget package\n. It was built on Visual Studio 2015 with .NET Framework 4.5.2. The easiest way to install is to use the \nnuget package manager\n in Visual Studio. As an other option the necessary dll can be built from the source code (\ndownload link\n). The current release is the v1.1. It has only one dependency: \nNewtonsoft.Json\n.",
            "title": "Installation"
        },
        {
            "location": "/install/#installation",
            "text": "NNSharp is available as a  nuget package . It was built on Visual Studio 2015 with .NET Framework 4.5.2. The easiest way to install is to use the  nuget package manager  in Visual Studio. As an other option the necessary dll can be built from the source code ( download link ). The current release is the v1.1. It has only one dependency:  Newtonsoft.Json .",
            "title": "Installation"
        },
        {
            "location": "/models/",
            "text": "Models\n\n\nInterface: IModel\n\n\nThis is the interface for all of the models. It contains only one function which returns the structure of the model. This is important to get access to the model details.\n\n\nSequential model\n\n\n \n[source]\n \n\n\n\nThe smallest building block of the sequential model is the \nlayer\n. The layers are organised consequtively. The output of a layer is the input to the next layer.\n\n\nNNSharp architecture is based on general descriptors of the possible operations (called kernels in this context) and then it applies a compiler to get the executable model. The compilation is done when a reader function (see Keras part for example) is called. The reader function reads the descriptors, builds a model then compiles it. After compilation calculations on new date reuires to call:\n\n\nIData ExecuteNetwork(IData input)\n\n\n\nWhere IData can be a Data2D data type (or later Data3D). At the detailed descriptions the exact meaning of the data format for the particular operation is described. \n\n\nDimension GetInputDimension()\n\n\n\nGives a structure, Dimension, containing the dimension of the expected input data.\n\n\nIModelData GetSummary()\n\n\n\nGives an object of \nSequentialModelData\n type. Therefore it is necessary to cast the output for that type. For the details of \nSequentialModelData\n see the documentation's \nData structure\n part.\n\n\nGraph model",
            "title": "Models"
        },
        {
            "location": "/models/#models",
            "text": "",
            "title": "Models"
        },
        {
            "location": "/models/#interface-imodel",
            "text": "This is the interface for all of the models. It contains only one function which returns the structure of the model. This is important to get access to the model details.",
            "title": "Interface: IModel"
        },
        {
            "location": "/models/#sequential-model",
            "text": "[source]    \nThe smallest building block of the sequential model is the  layer . The layers are organised consequtively. The output of a layer is the input to the next layer.  NNSharp architecture is based on general descriptors of the possible operations (called kernels in this context) and then it applies a compiler to get the executable model. The compilation is done when a reader function (see Keras part for example) is called. The reader function reads the descriptors, builds a model then compiles it. After compilation calculations on new date reuires to call:  IData ExecuteNetwork(IData input)  Where IData can be a Data2D data type (or later Data3D). At the detailed descriptions the exact meaning of the data format for the particular operation is described.   Dimension GetInputDimension()  Gives a structure, Dimension, containing the dimension of the expected input data.  IModelData GetSummary()  Gives an object of  SequentialModelData  type. Therefore it is necessary to cast the output for that type. For the details of  SequentialModelData  see the documentation's  Data structure  part.",
            "title": "Sequential model"
        },
        {
            "location": "/models/#graph-model",
            "text": "",
            "title": "Graph model"
        },
        {
            "location": "/data/",
            "text": "[source]\n \n\n\nData2D\n\n\nThis is a data type for stroring data with 2 dimensions like images. It has 2 further dimensions for storing the channels and batches. Therefore this type has four indicies with the following meaning: (height, width, channel, batch). The available functions:\n\n\nvoid ApplyToAll(Operation op)\n\n\n\nOperation is a function which receives a \ndouble\n and gives back a \ndouble\n. The operation will be applied on each element in the data independently.\n\n\nvoid ToZeros()\n\n\n\nAll of the elements become zero after applying this function.\n\n\nDimension GetDimension()\n\n\n\nThe Dimension is a structure with the following public attributes \nh\n, \nw\n, \nc\n, \nb\n.\n\n\n \n[source]\n \n\n\nDataArray\n\n\nThis is a special array. The data is accessable by one index. It has the following functions:\n\n\nvoid ApplyToAll(Operation op)\n\n\n\nThe same as before.\n\n\nvoid ToZeros()\n\n\n\nThe same as before.\n\n\nint GetLength()\n\n\n\nThe length of the array.\n\n\nThis array is enumerable as well.\n\n\n \n[source]\n \n\n\nSequentialModelData\n\n\nequentialModelData has the following methods to access information about the layers and the model:\n\n\nint GetNumberofLayers()\n\n\n\nGives the number of layers in the model. The InputLayer is not taken into account.\n\n\nstring GetLayerNameAt(int idx)\n\n\n\nGives the name of the layer at place \nidx\n. The counting starts at zero.\n\n\ndouble GetExecutionTime()\n\n\n\nGives the evaluation time of the network for one forward pass in \nmilli seconds\n.\n\n\nstring GetStringRepresentation()\n\n\n\nGives a string containing all the information of the layers.\n\n\nLayerData GetLayerDataAt(int idx)\n\n\n\nGives the LayerData for a concrete layer. LayerData contains the parameters of the input and the output data. The name of the layer is also available.",
            "title": "Data structure"
        },
        {
            "location": "/data/#data2d",
            "text": "This is a data type for stroring data with 2 dimensions like images. It has 2 further dimensions for storing the channels and batches. Therefore this type has four indicies with the following meaning: (height, width, channel, batch). The available functions:  void ApplyToAll(Operation op)  Operation is a function which receives a  double  and gives back a  double . The operation will be applied on each element in the data independently.  void ToZeros()  All of the elements become zero after applying this function.  Dimension GetDimension()  The Dimension is a structure with the following public attributes  h ,  w ,  c ,  b .    [source]",
            "title": "Data2D"
        },
        {
            "location": "/data/#dataarray",
            "text": "This is a special array. The data is accessable by one index. It has the following functions:  void ApplyToAll(Operation op)  The same as before.  void ToZeros()  The same as before.  int GetLength()  The length of the array.  This array is enumerable as well.    [source]",
            "title": "DataArray"
        },
        {
            "location": "/data/#sequentialmodeldata",
            "text": "equentialModelData has the following methods to access information about the layers and the model:  int GetNumberofLayers()  Gives the number of layers in the model. The InputLayer is not taken into account.  string GetLayerNameAt(int idx)  Gives the name of the layer at place  idx . The counting starts at zero.  double GetExecutionTime()  Gives the evaluation time of the network for one forward pass in  milli seconds .  string GetStringRepresentation()  Gives a string containing all the information of the layers.  LayerData GetLayerDataAt(int idx)  Gives the LayerData for a concrete layer. LayerData contains the parameters of the input and the output data. The name of the layer is also available.",
            "title": "SequentialModelData"
        },
        {
            "location": "/startkeras/",
            "text": "Getting Started with Keras models\n\n\nIf the training was done with Keras then the model can be saved by using the python script named \nKerasModeltoJSON.py\n. It will create a json file which can be read by NNSharp. NNSharp will build the model in C# and provides function to execute it. The python script takes the created and compiled Keras \nmodel\n and the \noutput file name\n as arguments. Then the json file can be created in the following way in your Python program:\n\n\nimport KerasModeltoJSON as js\nwrt = js.JSONwriter(model, file_path)\nwrt.save()\n\n\n\n\nThen in the C# program use the following:\n\n\n// Read the previously created json.\nvar reader = new ReaderKerasModel(filePath); \nSequentialModel model = reader.GetSequentialExecutor();\n\n// Then create the data to run the executer on.\n// batch: should be set in the Keras model.\nData2D input = new Data2D(height, width, channel, batch);\n\n// Calculate the network's output.\nIData output = model.ExecuteNetwork(input);\n\n\n\n\nIn order to know what data should be the \ninput\n and the \noutput\n see the corresponding documentations for the \nlayer at input\n and the \nlayer at output\n, respectively.",
            "title": "Quick start"
        },
        {
            "location": "/startkeras/#getting-started-with-keras-models",
            "text": "If the training was done with Keras then the model can be saved by using the python script named  KerasModeltoJSON.py . It will create a json file which can be read by NNSharp. NNSharp will build the model in C# and provides function to execute it. The python script takes the created and compiled Keras  model  and the  output file name  as arguments. Then the json file can be created in the following way in your Python program:  import KerasModeltoJSON as js\nwrt = js.JSONwriter(model, file_path)\nwrt.save()  Then in the C# program use the following:  // Read the previously created json.\nvar reader = new ReaderKerasModel(filePath); \nSequentialModel model = reader.GetSequentialExecutor();\n\n// Then create the data to run the executer on.\n// batch: should be set in the Keras model.\nData2D input = new Data2D(height, width, channel, batch);\n\n// Calculate the network's output.\nIData output = model.ExecuteNetwork(input);  In order to know what data should be the  input  and the  output  see the corresponding documentations for the  layer at input  and the  layer at output , respectively.",
            "title": "Getting Started with Keras models"
        },
        {
            "location": "/core/",
            "text": "The core layers contain layers to transform the shape of the input and the fully connected layer as a usual ingridient of a neural network. \n\n\n \n[source]\n \n\n\nFully connected (Dense layer)\n\n\nThe structure of \nFully connected layer\n can be seen on the following image:\n\n\n\n\nInput:\n\n\nA Data2D type with the shape: (1, 1, channels, batches).\n\n\nOutput:\n\n\nA Data2D type with the shape: (1, 1, units, batches).\n\n\nMethods:\n\n\nAs it can be seen on the picture this layer uses as many weight vectors (kernels) as many units have the layer. The number of units is equal with the number of output units. The fully connected layer is a linear transformation. A weight multiplies the corresponding input value (blue circle), then the output value (orange circle) is the sum of the previously weighted inputs. For further details see the source code where kernels are used in terms of units.\n\n\n \n[source]\n \n\n\nFlatten\n\n\nThe \nFlatten layer\n creates a 1-dimensional output (array) from the input. \n\n\nInput:\n\n\nA Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).\n\n\nOutput:\n\n\nA Data2D type data with the following shape: (1, 1, rows * columns * channels, batches). This represents a 1-dimensional vector.\n\n\nMethod:\n\n\nThe mapping occurs in the following way. The \nbatch\n remains unchanged. For a given \nbatch\n the reading order is: 1) channel, 2) column, 3) row. Therefore when \nrow\n and \ncolumn\n are fixed, \nchannel\n is changing. When \nchannel\n achieves its bound, \ncolumn\n is increased while \nrow\n is still fixed. Then \nchannel\n is iterated again and so on.\n\n\n \n[source]\n \n\n\nReshape2D\n\n\nThe \nReshape2D layer\n creates an output with the prescribed shape.\n\n\nInput:\n\n\nA Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).\n\n\nOutput:\n\n\nA Data2D type data with the required (prescribed) shape.\n\n\nMethod:\n\n\nReshaping assumes a strict reading order for accessing all of the elements. It has the following priority regarding the dimensions: 1) channel, 2) column, 3) row, 4) batch. If the elements are accessed by this manner, a one dimensional array (let's call it \nvirtual array\n) will contain all the elements. The algorithm will reshape the original data into a new one that after using the same access method on it, the same virtual array would appear. (The same element in the original and the new data should be mapped to the same place in the virtual array by the access method.)\n\n\n \n[source]\n \n\n\nPermute\n\n\nThe \nPermute layer\n creates an output with the same number of elements but the roles of the dimensions are changed.\n\n\nInput:\n\n\nA Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).\n\n\nOutput:\n\n\nA Data2D type data with permuted shape (new rows, new columns, new channels, batches). The \nbatches can not\n be changed by permutation. \n\n\nMethod:\n\n\nLet's suppose that the rows and channels are permuted in the output. Then an element in the input will be written to the permuted places as the example shows:\n\n\nIndicies in the input: h, w, c, b --> element = input[h, w, c, b].\nThen in the output data: output[c, w, h, b] = element.\n\n\n\n \n[source]\n \n\n\nRepeatVector\n\n\nThe \nRepeatVector layer\n repeats a 1-dimensional vector n times.\n\n\nInput:\n\n\nA Data2D type data with shape: (1, 1, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, n, channels, batches).\n\n\nMethod:\n\n\nRepeats the same 1-dimensional input for each channel and batch. \n\n\n \n[source]\n \n\n\nBias\n\n\nThe \nBias layer\n adds bias values to the input data. Elements with the same channel are increased by the same bias value. The input and the output shapes \nare the same\n.\n\n\nInput:\n\n\nA Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).\n\n\nOutput:\n\n\nIt is the same as the input.\n\n\nMethod:\n\n\nAdding bias  means the following:\n\n\noutput[h, w, c, b] = input[h, w, c, b] + bias[c]",
            "title": "Core"
        },
        {
            "location": "/core/#fully-connected-dense-layer",
            "text": "The structure of  Fully connected layer  can be seen on the following image:   Input:  A Data2D type with the shape: (1, 1, channels, batches).  Output:  A Data2D type with the shape: (1, 1, units, batches).  Methods:  As it can be seen on the picture this layer uses as many weight vectors (kernels) as many units have the layer. The number of units is equal with the number of output units. The fully connected layer is a linear transformation. A weight multiplies the corresponding input value (blue circle), then the output value (orange circle) is the sum of the previously weighted inputs. For further details see the source code where kernels are used in terms of units.    [source]",
            "title": "Fully connected (Dense layer)"
        },
        {
            "location": "/core/#flatten",
            "text": "The  Flatten layer  creates a 1-dimensional output (array) from the input.   Input:  A Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).  Output:  A Data2D type data with the following shape: (1, 1, rows * columns * channels, batches). This represents a 1-dimensional vector.  Method:  The mapping occurs in the following way. The  batch  remains unchanged. For a given  batch  the reading order is: 1) channel, 2) column, 3) row. Therefore when  row  and  column  are fixed,  channel  is changing. When  channel  achieves its bound,  column  is increased while  row  is still fixed. Then  channel  is iterated again and so on.    [source]",
            "title": "Flatten"
        },
        {
            "location": "/core/#reshape2d",
            "text": "The  Reshape2D layer  creates an output with the prescribed shape.  Input:  A Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).  Output:  A Data2D type data with the required (prescribed) shape.  Method:  Reshaping assumes a strict reading order for accessing all of the elements. It has the following priority regarding the dimensions: 1) channel, 2) column, 3) row, 4) batch. If the elements are accessed by this manner, a one dimensional array (let's call it  virtual array ) will contain all the elements. The algorithm will reshape the original data into a new one that after using the same access method on it, the same virtual array would appear. (The same element in the original and the new data should be mapped to the same place in the virtual array by the access method.)    [source]",
            "title": "Reshape2D"
        },
        {
            "location": "/core/#permute",
            "text": "The  Permute layer  creates an output with the same number of elements but the roles of the dimensions are changed.  Input:  A Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).  Output:  A Data2D type data with permuted shape (new rows, new columns, new channels, batches). The  batches can not  be changed by permutation.   Method:  Let's suppose that the rows and channels are permuted in the output. Then an element in the input will be written to the permuted places as the example shows:  Indicies in the input: h, w, c, b --> element = input[h, w, c, b].\nThen in the output data: output[c, w, h, b] = element.    [source]",
            "title": "Permute"
        },
        {
            "location": "/core/#repeatvector",
            "text": "The  RepeatVector layer  repeats a 1-dimensional vector n times.  Input:  A Data2D type data with shape: (1, 1, channels, batches).  Output:  A Data2D type data with shape: (1, n, channels, batches).  Method:  Repeats the same 1-dimensional input for each channel and batch.     [source]",
            "title": "RepeatVector"
        },
        {
            "location": "/core/#bias",
            "text": "The  Bias layer  adds bias values to the input data. Elements with the same channel are increased by the same bias value. The input and the output shapes  are the same .  Input:  A Data2D type data with arbitrary shape (rows, columns, channels, batches can be anything).  Output:  It is the same as the input.  Method:  Adding bias  means the following:  output[h, w, c, b] = input[h, w, c, b] + bias[c]",
            "title": "Bias"
        },
        {
            "location": "/convolutions/",
            "text": "[source]\n \n\n\nConvolution1D\n\n\n1-dimensional convolution is a special case of a 2-dimensional convolution. It processes the input with a sliding window (called kernel).\n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, new length, kernel numbers, batches). \n\n\nThe shape of the filter: (1, kernel size, channels, kernel numbers). Then the output length (new  length):\n\nnew length = 1 + (length + 2 * padding - kernel size)/stride\n.\n\n\nMethods:\n\n\nThe 1-dimensional convolution applies a 1-diemsional kernel with the same number of channels than the input has. Then the kernel moves around the input vector and multiplies the corresponding elements then the results of the multiplications are summed up. The kernel starts at the left side of the input tensor. If \npadding\n is different than 0, the input tensor is virtually extended by some padding value (usually 0) at the left and right sides. The kernel moves some places ahead to the direction of the right side. The number of places are defined by the \nstride\n. The formula of the calculation: \noutput(i) = sum_c(sum_k(input(i * stride - padding + k, c) * kernel(k, c)))\n, where i is the place in the output, k is the index of an element, c is the index of the channel. \n\n\n \n[source]\n \n\n\nConvolution2D\n\n\nAn illustration of the calculation for a given position of the kernel.\n\n\n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (new height, new width, kernel numbers, batches). \n\n\nThe shape of the filter: (kernel height, kernel width, channels, kernel numbers). Then the output sizes:\n\nnew height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical\n,\n\nnew width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal\n.\n\n\nMethods:\n\n\nThe same as the one dimensional but the sliding happens in the second dimension as well. The formula:\n\nx = i * strideHZ - paddingHZ + kHZ\n \n\n\ny = j * strideVR - paddingVR + kVR\n\n\noutput(i,j) = sum_c(sum_kHZ(sum_kVR((x, y, c) * kernel(i, j, c))))\n\n\n \n[source]\n \n\n\nCropping1D\n\n\nTrims some elements at the beginning and at the end.\n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, length - trimmed, channels, batches).\n\n\n \n[source]\n \n\n\nCropping2D\n\n\nTrims elements at the top, bottom, left and right sides. \n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (height - trimmedA, width - trimmedB, channels, batches).\n\n\ntrimmedA is the overall trimmed rows at the top and bottom. \n\ntrimmedB is the overall trimmed columns at the left and right.",
            "title": "Convolutions"
        },
        {
            "location": "/convolutions/#convolution1d",
            "text": "1-dimensional convolution is a special case of a 2-dimensional convolution. It processes the input with a sliding window (called kernel).  Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, new length, kernel numbers, batches).   The shape of the filter: (1, kernel size, channels, kernel numbers). Then the output length (new  length): new length = 1 + (length + 2 * padding - kernel size)/stride .  Methods:  The 1-dimensional convolution applies a 1-diemsional kernel with the same number of channels than the input has. Then the kernel moves around the input vector and multiplies the corresponding elements then the results of the multiplications are summed up. The kernel starts at the left side of the input tensor. If  padding  is different than 0, the input tensor is virtually extended by some padding value (usually 0) at the left and right sides. The kernel moves some places ahead to the direction of the right side. The number of places are defined by the  stride . The formula of the calculation:  output(i) = sum_c(sum_k(input(i * stride - padding + k, c) * kernel(k, c))) , where i is the place in the output, k is the index of an element, c is the index of the channel.     [source]",
            "title": "Convolution1D"
        },
        {
            "location": "/convolutions/#convolution2d",
            "text": "An illustration of the calculation for a given position of the kernel.   Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (new height, new width, kernel numbers, batches).   The shape of the filter: (kernel height, kernel width, channels, kernel numbers). Then the output sizes: new height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical , new width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal .  Methods:  The same as the one dimensional but the sliding happens in the second dimension as well. The formula: x = i * strideHZ - paddingHZ + kHZ    y = j * strideVR - paddingVR + kVR  output(i,j) = sum_c(sum_kHZ(sum_kVR((x, y, c) * kernel(i, j, c))))    [source]",
            "title": "Convolution2D"
        },
        {
            "location": "/convolutions/#cropping1d",
            "text": "Trims some elements at the beginning and at the end.  Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, length - trimmed, channels, batches).    [source]",
            "title": "Cropping1D"
        },
        {
            "location": "/convolutions/#cropping2d",
            "text": "Trims elements at the top, bottom, left and right sides.   Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (height - trimmedA, width - trimmedB, channels, batches).  trimmedA is the overall trimmed rows at the top and bottom.  \ntrimmedB is the overall trimmed columns at the left and right.",
            "title": "Cropping2D"
        },
        {
            "location": "/pooling/",
            "text": "The pooling layers similar to the convolutional layers except an important difference: the pooling occurs for the channels separately.\n\n\n[source]\n\n\nAveragePooling1D\n\n\nThis layer calculates the average value in the kernel (sliding window). \n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, new length, channels, batches). \n\n\nThe shape of the filter: (1, kernel size, 1, 1). Then the output length (new  length):\n\nnew length = 1 + (length + 2 * padding - kernel size)/stride\n.\n\n\n[source]\n\n\nAveragePooling2D\n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (new height, new width, channels, batches). \n\n\nThe shape of the filter: (kernel height, kernel width, 1, 1). Then the output sizes:\n\nnew height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical\n,\n\nnew width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal\n.\n\n\n[source]\n\n\nGlobalAveragePooling1D\n\n\nCalculates the average value for each channel in the input data. \n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, 1, channels, batches). \n\n\n[source]\n\n\nGlobalAveragePooling2D\n\n\nCalculates the average value for each channel in the input data. \n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, 1, channels, batches).\n\n\n[source]\n\n\nGlobalMaxPooling1D\n\n\nCalculates the maximum value for each channel in the input data. \n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, 1, channels, batches). \n\n\n[source]\n\n\nGlobalMaxPooling2D\n\n\nCalculates the maximum value for each channel in the input data. \n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, 1, channels, batches).\n\n\n[source]\n\n\nMaxPooling1D\n\n\nThis layer calculates the maximum value in the kernel (sliding window). \n\n\nInput:\n\n\nA Data2D type data with shape: (1, length, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (1, new length, channels, batches). \n\n\nThe shape of the filter: (1, kernel size, 1, 1). Then the output length (new  length):\n\nnew length = 1 + (length + 2 * padding - kernel size)/stride\n.\n\n\n[source]\n\n\nMaxPooling2D\n\n\nThis layer calculates the maximum value in the kernel (sliding window).\n\n\nInput:\n\n\nA Data2D type data with shape: (height, width, channels, batches).\n\n\nOutput:\n\n\nA Data2D type data with shape: (new height, new width, channels, batches). \n\n\nThe shape of the filter: (kernel height, kernel width, 1, 1). Then the output sizes:\n\nnew height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical\n,\n\nnew width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal\n.",
            "title": "Pooling"
        },
        {
            "location": "/pooling/#averagepooling1d",
            "text": "This layer calculates the average value in the kernel (sliding window).   Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, new length, channels, batches).   The shape of the filter: (1, kernel size, 1, 1). Then the output length (new  length): new length = 1 + (length + 2 * padding - kernel size)/stride .  [source]",
            "title": "AveragePooling1D"
        },
        {
            "location": "/pooling/#averagepooling2d",
            "text": "Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (new height, new width, channels, batches).   The shape of the filter: (kernel height, kernel width, 1, 1). Then the output sizes: new height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical , new width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal .  [source]",
            "title": "AveragePooling2D"
        },
        {
            "location": "/pooling/#globalaveragepooling1d",
            "text": "Calculates the average value for each channel in the input data.   Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, 1, channels, batches).   [source]",
            "title": "GlobalAveragePooling1D"
        },
        {
            "location": "/pooling/#globalaveragepooling2d",
            "text": "Calculates the average value for each channel in the input data.   Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (1, 1, channels, batches).  [source]",
            "title": "GlobalAveragePooling2D"
        },
        {
            "location": "/pooling/#globalmaxpooling1d",
            "text": "Calculates the maximum value for each channel in the input data.   Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, 1, channels, batches).   [source]",
            "title": "GlobalMaxPooling1D"
        },
        {
            "location": "/pooling/#globalmaxpooling2d",
            "text": "Calculates the maximum value for each channel in the input data.   Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (1, 1, channels, batches).  [source]",
            "title": "GlobalMaxPooling2D"
        },
        {
            "location": "/pooling/#maxpooling1d",
            "text": "This layer calculates the maximum value in the kernel (sliding window).   Input:  A Data2D type data with shape: (1, length, channels, batches).  Output:  A Data2D type data with shape: (1, new length, channels, batches).   The shape of the filter: (1, kernel size, 1, 1). Then the output length (new  length): new length = 1 + (length + 2 * padding - kernel size)/stride .  [source]",
            "title": "MaxPooling1D"
        },
        {
            "location": "/pooling/#maxpooling2d",
            "text": "This layer calculates the maximum value in the kernel (sliding window).  Input:  A Data2D type data with shape: (height, width, channels, batches).  Output:  A Data2D type data with shape: (new height, new width, channels, batches).   The shape of the filter: (kernel height, kernel width, 1, 1). Then the output sizes: new height = 1 + (height + 2 * padding vertical - kernel height)/stride vertical , new width = 1 + (width + 2 * padding horizontal - kernel width)/stride horizontal .",
            "title": "MaxPooling2D"
        },
        {
            "location": "/activations/",
            "text": "Activation functions are applied in an element-wise maner on the input. \nTherefore the dimension of the input and output is the same.\n The goal of the activation functions is to break the linearity of the network. This can provide a function space (in terms of the trainable weights) which can approximate a large body of mapping rules from the inputs to the outputs. For mathematical details see the following references: \n\n\n\n\nFunahashi, K. I. \"On the Approximate Realization of Continuous Mappings by Neural Networks\", Neural Networks, Vol. 2. No. 3. pp. 183-192. 1989.\n\n\nLeshno, M. - Lin, V. Y. - Pinkus, A. - Schocken, S. \"Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function\", Neural Networks, Vol. 6. pp. 861-867. 1993.\n\n\n\n\nThe last one contains the following \ntheorem\n:\n\n\nAssume the neural network contains one non-linear layer. An \nf\n  function (domain and range are real numbers) can be approximated with arbitrary accuracy if and only if the applied activation function is not a polynom and it is locally bounded.\n\n\n \n[source]\n \n\n\nELu\n\n\n\n\n \n[source]\n \n\n\nHardSigmoid\n\n\n\n\n \n[source]\n \n\n\nReLu\n\n\n\n\n \n[source]\n \n\n\nSigmoid\n\n\n\n\n \n[source]\n \n\n\nSoftPlus\n\n\n\n\n \n[source]\n \n\n\nSoftmax\n\n\nSupports only 1 dimensional inputs.\n\n\n\n\n \n[source]\n \n\n\nSoftsign\n\n\n\n\n \n[source]\n \n\n\nTanH",
            "title": "Activations"
        },
        {
            "location": "/activations/#elu",
            "text": "[source]",
            "title": "ELu"
        },
        {
            "location": "/activations/#hardsigmoid",
            "text": "[source]",
            "title": "HardSigmoid"
        },
        {
            "location": "/activations/#relu",
            "text": "[source]",
            "title": "ReLu"
        },
        {
            "location": "/activations/#sigmoid",
            "text": "[source]",
            "title": "Sigmoid"
        },
        {
            "location": "/activations/#softplus",
            "text": "[source]",
            "title": "SoftPlus"
        },
        {
            "location": "/activations/#softmax",
            "text": "Supports only 1 dimensional inputs.     [source]",
            "title": "Softmax"
        },
        {
            "location": "/activations/#softsign",
            "text": "[source]",
            "title": "Softsign"
        },
        {
            "location": "/activations/#tanh",
            "text": "",
            "title": "TanH"
        },
        {
            "location": "/recurrents/",
            "text": "The main idea behind a recurrent neural network (RNN) is to grab the relations in a process where the input is a time-sequence with consequtive data pieces. Therefore the order of the pieces is strict. The relation connects to the time. An examplary problem can be when a software tries to predict the next data piece by knowing the previous 5 pieces. \n\n\nGenerally the input of an rnn is a set of vectors (e.g.: a vector for each time point) and the output is also a set of vector but may be it has different size then the input. Each vector is processed by a cell (only one type of cell is applied) but in a strict order and with some delay. Therefore as the computation goes each cell processes a vector, gives (or not) an output and calculates some further state values which will be fed into the next cell. The next cell receives the state values and the next input vector from the sequence, executes the calculations and so on. \n\n\nShortly, an RNN is a finite long sequence of cells. Each cell has the same structure. The input vectors processed consequtively and some hidden variables are computed and forwarded as well. Some references for further insight:\n\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks\n\n\nA Beginner\u2019s Guide to Recurrent Networks and LSTMs\n\n\nRecurrent Neural Networks Tutorial on WILDML\n.\n\n\n[source]\n\n\nSimpleRNN\n\n\nThe structure of the so called simple RNN layer in Keras is the following:\n\n\n\n\nThe \nh\n is the hidden variable, \nR\n is the recurrent kernel, \nW\n is the kernel, \nb\n is the bias and \nf\n is the activation function. \n\n\nInput:\n\n\nA Data2D type with the shape: (1, timesteps, input dimension, batches).\n\n\nOutput:\n\n\nA Data2D type with shape: (1, 1, units, batches). \n\n\nMethods:\n\n\nThe SimpleRNN cell implements the operations shown in the picture and feeds the output to the next cell. The unrolled cells are implemented by a \nfor\n cycle. It can be useful to inspect the \nKeras implementation\n as well.\n\n\n[source]\n\n\nLSTM\n\n\nThe structure of the LSTM cell is the following:\n\n\n\n\nThe \ng\n is the recurrent activation, \np\n is the activation, \nW\ns are the kernels, \nU\ns are the recurrent kernels, \nh\n is the hidden variable which is the output too and the notation * is an element-wise multiplication.\n\n\nInput:\n\n\nA Data2D type with the shape: (1, timesteps, input dimension, batches).\n\n\nOutput:\n\n\nA Data2D type with shape: (1, 1, units, batches).\n\n\nMethods:\n\n\nThe Keras implementation can help as well: \nsee\n the step function in the LSTM implementation. Basically the products are dot products between matricies.\n\n\n[source]\n\n\nGRU\n\n\nThe structure of the GRU cell is the following:\n\n\n\n\nThe meaning of the notations are the same as in case of LSTM. 1-z means an element-wise subtraction. \n\n\nInput:\n\n\nA Data2D type with the shape: (1, timesteps, input dimension, batches).\n\n\nOutput:\n\n\nA Data2D type with shape: (1, 1, units, batches).\n\n\nMethods:\n\n\nThe Keras implementation can help as well: \nsee\n the step function in the LSTM implementation. \nThis\n blog article can be useful as well.",
            "title": "Recurrent layers"
        },
        {
            "location": "/recurrents/#simplernn",
            "text": "The structure of the so called simple RNN layer in Keras is the following:   The  h  is the hidden variable,  R  is the recurrent kernel,  W  is the kernel,  b  is the bias and  f  is the activation function.   Input:  A Data2D type with the shape: (1, timesteps, input dimension, batches).  Output:  A Data2D type with shape: (1, 1, units, batches).   Methods:  The SimpleRNN cell implements the operations shown in the picture and feeds the output to the next cell. The unrolled cells are implemented by a  for  cycle. It can be useful to inspect the  Keras implementation  as well.  [source]",
            "title": "SimpleRNN"
        },
        {
            "location": "/recurrents/#lstm",
            "text": "The structure of the LSTM cell is the following:   The  g  is the recurrent activation,  p  is the activation,  W s are the kernels,  U s are the recurrent kernels,  h  is the hidden variable which is the output too and the notation * is an element-wise multiplication.  Input:  A Data2D type with the shape: (1, timesteps, input dimension, batches).  Output:  A Data2D type with shape: (1, 1, units, batches).  Methods:  The Keras implementation can help as well:  see  the step function in the LSTM implementation. Basically the products are dot products between matricies.  [source]",
            "title": "LSTM"
        },
        {
            "location": "/recurrents/#gru",
            "text": "The structure of the GRU cell is the following:   The meaning of the notations are the same as in case of LSTM. 1-z means an element-wise subtraction.   Input:  A Data2D type with the shape: (1, timesteps, input dimension, batches).  Output:  A Data2D type with shape: (1, 1, units, batches).  Methods:  The Keras implementation can help as well:  see  the step function in the LSTM implementation.  This  blog article can be useful as well.",
            "title": "GRU"
        },
        {
            "location": "/batchnormalization/",
            "text": "[source]\n \n\n\nBatch Normalization\n\n\nThe intuition behind Batch Normalization is that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This tends to slow down the learning process. Normalization transforms the input data in a way that its mean becomes 0 and its standard deviation becomes 1. For further details see the original article:\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n.\n\n\nInput:\n\n\nA Data2D array with arbitrary shape.\n\n\nOutput:\n\n\nThe same as the input. The numbers are normalized in the channels separately. \n\n\nMethods:\n\n\nThe following equations are implemented. \nThe normalization is performed according to the feature (channel) axis.\n\n\n\n\nThe \ngamma\n, \nbeta\n are parameters to learn, \nsigma\n is the variance, \nb\n is the bias. The \nepsilon\n is a small number to avoid the problem with small variances. (Ensures numerical stability.)",
            "title": "BatchNormalization"
        },
        {
            "location": "/batchnormalization/#batch-normalization",
            "text": "The intuition behind Batch Normalization is that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This tends to slow down the learning process. Normalization transforms the input data in a way that its mean becomes 0 and its standard deviation becomes 1. For further details see the original article:  Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  Input:  A Data2D array with arbitrary shape.  Output:  The same as the input. The numbers are normalized in the channels separately.   Methods:  The following equations are implemented.  The normalization is performed according to the feature (channel) axis.   The  gamma ,  beta  are parameters to learn,  sigma  is the variance,  b  is the bias. The  epsilon  is a small number to avoid the problem with small variances. (Ensures numerical stability.)",
            "title": "Batch Normalization"
        },
        {
            "location": "/starttensorflow/",
            "text": "(Next release)",
            "title": "Quick start"
        }
    ]
}