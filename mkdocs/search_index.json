{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to NNSharp\n\n\nThis is the full documentation of NNSharp which is a lightweight library for running pre-trained neural networks. The training should be done in \nKeras\n, \nPytorch\n or \nSonnet\n then the weights and the network architecture should be saved into a file (json). NNSharp is able to read and run the network especially on Windows and Visual Studio. \n\n\nCurrent abilities\n\n\nNNSharp recently supports Keras models with both Tensorflow and Theano backend. The list of the supported \nKeras layers\n:\n\n\n\n\nCore layers\n: Dense, Reshape (2D), Permute, RepeatVector. \n\n\nConvolution layers\n: Conv1D, Conv2D, Cropping1D, Cropping2D. \n\n\nPooling layers\n: AveragePooling1D, AveragePooling2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D.\n\n\nActivations\n: softmax, elu, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid. \n\n\nNormalization\n: BatchNormalization.\n\n\n\n\nThe \ndata format\n supports 2-dimensional data. \n\n\nThe structure of the documentation\n\n\nThe pusrpose of the documentation is twofold:\n\n\n\n\nAs a user guide to show how to use the package.\n\n\nProviding some further insight how the kernels work in a concise manner. Therefore developers and researchers can immediately understand what the kernel does without inspecting the code or looking for other sources on the net. This can be helpful for contributors as a developer documentation. On the other hand users can understand what to feed into the network and how to use (and construe) the output.\n\n\n\n\nThe structure of the documentation is the following:\n\n\n\n\nInstallation\n shows how to get access to the package.\n\n\nModels\n introduces the model types and methods to access information about the model like the contained layers or nodes, execution time etc.\n\n\nKeras\n shows the implemented Keras layers. Each layer introduced with its input, output and its underlying process.\n\n\nTensorFlow\n shows how to use pre-trained TensorFlow models in NNSharp.\n\n\n\n\nFuture\n\n\nIn the future the library should provide the following features:\n\n\n\n\nSupports the whole Keras API.\n\n\nSupports PyTorch regarding neural netowrks.\n\n\nSupports Sonnet.\n\n\nMulti-threading for faster kernels.\n\n\nKeras-like API over TensorFlow for building, training and running networks. This requires an extended C++ API in TensorFlow and reliable compilation for Windows.\n\n\n\n\nContributions are always welcomed! For the current purposes see the \ngithub repository\n of the project.",
            "title": "Overview"
        },
        {
            "location": "/#welcome-to-nnsharp",
            "text": "This is the full documentation of NNSharp which is a lightweight library for running pre-trained neural networks. The training should be done in  Keras ,  Pytorch  or  Sonnet  then the weights and the network architecture should be saved into a file (json). NNSharp is able to read and run the network especially on Windows and Visual Studio.",
            "title": "Welcome to NNSharp"
        },
        {
            "location": "/#current-abilities",
            "text": "NNSharp recently supports Keras models with both Tensorflow and Theano backend. The list of the supported  Keras layers :   Core layers : Dense, Reshape (2D), Permute, RepeatVector.   Convolution layers : Conv1D, Conv2D, Cropping1D, Cropping2D.   Pooling layers : AveragePooling1D, AveragePooling2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D.  Activations : softmax, elu, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid.   Normalization : BatchNormalization.   The  data format  supports 2-dimensional data.",
            "title": "Current abilities"
        },
        {
            "location": "/#the-structure-of-the-documentation",
            "text": "The pusrpose of the documentation is twofold:   As a user guide to show how to use the package.  Providing some further insight how the kernels work in a concise manner. Therefore developers and researchers can immediately understand what the kernel does without inspecting the code or looking for other sources on the net. This can be helpful for contributors as a developer documentation. On the other hand users can understand what to feed into the network and how to use (and construe) the output.   The structure of the documentation is the following:   Installation  shows how to get access to the package.  Models  introduces the model types and methods to access information about the model like the contained layers or nodes, execution time etc.  Keras  shows the implemented Keras layers. Each layer introduced with its input, output and its underlying process.  TensorFlow  shows how to use pre-trained TensorFlow models in NNSharp.",
            "title": "The structure of the documentation"
        },
        {
            "location": "/#future",
            "text": "In the future the library should provide the following features:   Supports the whole Keras API.  Supports PyTorch regarding neural netowrks.  Supports Sonnet.  Multi-threading for faster kernels.  Keras-like API over TensorFlow for building, training and running networks. This requires an extended C++ API in TensorFlow and reliable compilation for Windows.   Contributions are always welcomed! For the current purposes see the  github repository  of the project.",
            "title": "Future"
        },
        {
            "location": "/install/",
            "text": "Installation\n\n\nNNSharp is available as a \nnuget package\n. It was built on Visual Studio 2015 with .NET Framework 4.5.2. The easiest way to install is to use the \nnuget package manager\n in Visual Studio. As an other option the necessary dll can be built from the source code (\ndownload link\n). The current release is the v1.1. It has only one dependency: \nNewtonsoft.Json\n.",
            "title": "Installation"
        },
        {
            "location": "/install/#installation",
            "text": "NNSharp is available as a  nuget package . It was built on Visual Studio 2015 with .NET Framework 4.5.2. The easiest way to install is to use the  nuget package manager  in Visual Studio. As an other option the necessary dll can be built from the source code ( download link ). The current release is the v1.1. It has only one dependency:  Newtonsoft.Json .",
            "title": "Installation"
        },
        {
            "location": "/models/",
            "text": "Models\n\n\nInterface: IModel\n\n\nThis is the interface for all of the models. It contains only one function which returns the structure of the model. This is important to get access to the model details.\n\n\nSequential model\n\n\n \n[source]\n \n\n\n\nThe smallest building block of the sequential model is the \nlayer\n. The layers are organised consequtively. The output of a layer is the input to the next layer.\n\n\nNNSharp architecture is based on general descriptors of the possible operations (called kernels in this context) and then it applies a compiler to get the executable model. The compilation is done when a reader function (see Keras part for example) is called. The reader function reads the descriptors, builds a model then compiles it. After compilation calculations on new date reuires to call:\n\n\nIData ExecuteNetwork(IData input)\n\n\n\nWhere IData can be a Data2D data type (or later Data3D). At the detailed descriptions the exact meaning of the data format for the particular operation is described. \n\n\nDimension GetInputDimension()\n\n\n\nGives a structure, Dimension, containing the dimension of the expected input data.\n\n\nIModelData GetSummary()\n\n\n\nGives an object of \nSequentialModelData\n type. Therefore it is necessary to cast the output for that type. SequentialModelData has the following methods to access information about the layers and the model:\n\n\nint GetNumberofLayers()\n\n\n\nGives the number of layers in the model. The InputLayer is not taken into account.\n\n\nstring GetLayerNameAt(int idx)\n\n\n\nGives the name of the layer at place \nidx\n. The counting starts at zero.\n\n\ndouble GetExecutionTime()\n\n\n\nGives the evaluation time of the network for one forward pass in \nmilli seconds\n.\n\n\nstring GetStringRepresentation()\n\n\n\nGives a string containing all the information of the layers.\n\n\nLayerData GetLayerDataAt(int idx)\n\n\n\nGives the LayerData for a concrete layer. LayerData contains the parameters of the input and the output data. The name of the layer is also available.\n\n\nGraph model",
            "title": "Models"
        },
        {
            "location": "/models/#models",
            "text": "",
            "title": "Models"
        },
        {
            "location": "/models/#interface-imodel",
            "text": "This is the interface for all of the models. It contains only one function which returns the structure of the model. This is important to get access to the model details.",
            "title": "Interface: IModel"
        },
        {
            "location": "/models/#sequential-model",
            "text": "[source]    \nThe smallest building block of the sequential model is the  layer . The layers are organised consequtively. The output of a layer is the input to the next layer.  NNSharp architecture is based on general descriptors of the possible operations (called kernels in this context) and then it applies a compiler to get the executable model. The compilation is done when a reader function (see Keras part for example) is called. The reader function reads the descriptors, builds a model then compiles it. After compilation calculations on new date reuires to call:  IData ExecuteNetwork(IData input)  Where IData can be a Data2D data type (or later Data3D). At the detailed descriptions the exact meaning of the data format for the particular operation is described.   Dimension GetInputDimension()  Gives a structure, Dimension, containing the dimension of the expected input data.  IModelData GetSummary()  Gives an object of  SequentialModelData  type. Therefore it is necessary to cast the output for that type. SequentialModelData has the following methods to access information about the layers and the model:  int GetNumberofLayers()  Gives the number of layers in the model. The InputLayer is not taken into account.  string GetLayerNameAt(int idx)  Gives the name of the layer at place  idx . The counting starts at zero.  double GetExecutionTime()  Gives the evaluation time of the network for one forward pass in  milli seconds .  string GetStringRepresentation()  Gives a string containing all the information of the layers.  LayerData GetLayerDataAt(int idx)  Gives the LayerData for a concrete layer. LayerData contains the parameters of the input and the output data. The name of the layer is also available.",
            "title": "Sequential model"
        },
        {
            "location": "/models/#graph-model",
            "text": "",
            "title": "Graph model"
        },
        {
            "location": "/startkeras/",
            "text": "",
            "title": "Quick start"
        },
        {
            "location": "/core/",
            "text": "Core layers\n\n\nThe core layers contains layers to transform the shape of the input and the fully connected layer as a usual ingridient of a neural networks. \n\n\nFully connected (Dense layer)\n\n\n \n[source]\n \n\n\nFlatten\n\n\n \n[source]\n \n\n\nReshape2D\n\n\n \n[source]\n \n\n\nPermute\n\n\n \n[source]\n \n\n\nRepeatVector\n\n\n \n[source]",
            "title": "Core"
        },
        {
            "location": "/core/#core-layers",
            "text": "The core layers contains layers to transform the shape of the input and the fully connected layer as a usual ingridient of a neural networks.",
            "title": "Core layers"
        },
        {
            "location": "/core/#fully-connected-dense-layer",
            "text": "[source]",
            "title": "Fully connected (Dense layer)"
        },
        {
            "location": "/core/#flatten",
            "text": "[source]",
            "title": "Flatten"
        },
        {
            "location": "/core/#reshape2d",
            "text": "[source]",
            "title": "Reshape2D"
        },
        {
            "location": "/core/#permute",
            "text": "[source]",
            "title": "Permute"
        },
        {
            "location": "/core/#repeatvector",
            "text": "[source]",
            "title": "RepeatVector"
        },
        {
            "location": "/convolutions/",
            "text": "Convolution like layers\n\n\nConvolution1D\n\n\n \n[source]\n\n\nThe convolution...",
            "title": "Convolutions"
        },
        {
            "location": "/convolutions/#convolution-like-layers",
            "text": "",
            "title": "Convolution like layers"
        },
        {
            "location": "/convolutions/#convolution1d",
            "text": "[source]  The convolution...",
            "title": "Convolution1D"
        },
        {
            "location": "/pooling/",
            "text": "",
            "title": "Pooling"
        },
        {
            "location": "/activations/",
            "text": "",
            "title": "Activations"
        },
        {
            "location": "/recurrents/",
            "text": "",
            "title": "Recurrent layers"
        },
        {
            "location": "/batchnormalization/",
            "text": "Normalization layer\n\n\nBatch Normalization \n[source]",
            "title": "BatchNormalization"
        },
        {
            "location": "/batchnormalization/#normalization-layer",
            "text": "",
            "title": "Normalization layer"
        },
        {
            "location": "/batchnormalization/#batch-normalization-source",
            "text": "",
            "title": "Batch Normalization [source]"
        },
        {
            "location": "/starttensorflow/",
            "text": "",
            "title": "Quick start"
        }
    ]
}